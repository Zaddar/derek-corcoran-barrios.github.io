---
title: "Regresión Penalizada"
author: "Derek Corcoran"
date: "`r format(Sys.time(), '%d/%m, %Y')`"
output:
  ioslides_presentation:
    widescreen: true
    incremental: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, tidy = TRUE, cache = F)
library(MuMIn)
library(tidyverse)
library(broom)
library(caret)
library(kableExtra)
library(glmnet)
library(patchwork)
options("kableExtra.html.bsTable" = T)
```

## Regresión penalizada

Empezemos con una regresión lineal simple:

```{r}
data("mtcars")
Fit <- lm(mpg ~ wt, data = mtcars)
```

```{r, echo = FALSE}
kable(broom::tidy(Fit), digits = 3) %>% kable_styling(bootstrap_options = c("striped", "hover"))
```

## Como obtenemos la pendiente y el intercepto? {.build}

* Usando mínimos cuadrados

$$\underset{SEE}{\text{minimize}}  = \sum_{i = 1}^n{(y_i - \hat{y_i})^2}$$

## Minimos cuadrados

```{r}
Preds <- augment(Fit)
ggplot(Preds, aes(x = wt, y = mpg)) + geom_point() + geom_path(aes(y = .fitted)) + geom_linerange(aes(ymin = .fitted, ymax = mpg)) + theme_bw()
```

## Suma de errores cuadrados

```{r}
SSE <- Preds %>% mutate(SE = .resid^2) %>% summarise(SSE = sum(SE))
```

```{r, echo = F}
kable(SSE, digits = 2) %>%  kable_paper("hover")
```

## Ejemplo:

```{r, echo = F, animation.hook="gifski", interval = 0.15}

MinSq <- data.frame(Beta = seq(0, -10, length.out = 80), MinSq = NA) %>% arrange(Beta)
mt <- mtcars

for(i in 1:nrow(MinSq)){
  mt$pred <- 30.1 + MinSq$Beta[i]*mt$wt
MinSq$MinSq[i] <- mt %>% mutate(resid_sq = (mpg - pred)^2) %>% summarise(MinSq = sum(resid_sq)) %>% pull(MinSq)

G1 <- ggplot(mt, aes(x = wt, y = mpg)) + geom_path(aes(y = pred)) + geom_linerange(aes(ymin = pred, ymax = mpg)) + geom_point() + theme_bw()

G2 <- ggplot(MinSq, aes(x = Beta, y = MinSq)) + geom_path() + ylim(c(400,17000))+ theme_bw() + geom_label(aes(x = -5, y = 12000, label = paste0("MinSq =",round(min(MinSq, na.rm = T),0)))) + geom_vline(data =(dplyr::filter(MinSq, MinSq == min(MinSq, na.rm = T))) ,aes(xintercept = Beta), lty = 2, color = "red")
Final <- G1 + G2
print(Final)
  }

```

## Cuando esto es un problema

* A veces nuestra poblacion es muy distinta a nuestra muestra

```{r, echo = F}
mt <- mtcars
set.seed(672)
mt_20 <- mt %>% dplyr::sample_frac(0.2)
Fit_20 <- lm(mpg ~ wt, data = mt_20)
  
mt$Pred <- predict(Fit_20, mt)

ggplot(mt, aes(x = wt, y = mpg)) + geom_path(aes(y = Pred)) + geom_point() + geom_point(data = mt_20, color = "red") + theme_bw()
```

## Parametros

```{r}
tidy(Fit_20) %>% kable %>% kable_styling(bootstrap_options = c("striped", "hover"))
```


## Penalizamos con $\lambda$

* Usando penalización

$$\underset{SEE}{\text{minimize}}  = \sum_{i = 1}^n{(y_i - \hat{y_i})^2} + \lambda(\beta_{}^2)$$

## Veamos como cambian los parametros:

```{r, animation.hook="gifski", interval = 0.1, echo = F}
x <- mt_20 %>% dplyr::select(wt) %>% mutate(New = 0) %>% as.matrix()
# Outcome variable
y <- mt_20 %>% pull(mpg)
Fit_Pen <- glmnet(x, y, alpha = 0)
DF <- Fit_Pen$beta %>% as.matrix() %>% t() %>% as.data.frame()
DF$a0 <- Fit_Pen$a0
DF$Lambda <- Fit_Pen$lambda
DF <- DF %>% filter(Lambda < 300) %>% arrange(Lambda)
for(i in 1:nrow(DF)){
  mt$Pred <- DF$a0[i] + mt$wt*DF$wt[i]
  DF$MinSq[i] <- mt %>% mutate(MinSq = (Pred - mpg)^2) %>% summarise(MinSq = sum(MinSq)) %>% pull(MinSq)
g <- ggplot(mt, aes(x = wt, y = mpg)) + geom_path(aes(y = Pred)) + geom_point() + geom_point(data = mt_20, color = "red") + theme_bw() + ggtitle(paste("Lambda =", round(DF$Lambda[i],2))) + ylim(c(-10, 35))
print(g)
}
```


## Seleccionando Lambda

```{r, echo = FALSE}
ggplot(DF, aes(x = Lambda, y = MinSq)) + geom_path() + theme_bw()
```

```{r, echo = F}
DF %>% dplyr::filter(MinSq == min(MinSq)) %>% select(-New) %>% kable(digits = 2) %>% kable_paper("hover")
```

# Nuestro primer hiperparámetro $\lambda$

## Usamos cross-validation para seleccionarlo

* **ridge:** alpha = 0, mejor en predicciones (todas las variables son útiles)

$$\underset{SEE}{\text{minimize}}  = \sum_{i = 1}^n{(y_i - \hat{y_i})^2} + \lambda(\beta_{}^2)$$

* **Lasso:** alpha = 1, selecciona variables (pueden haber variables inutiles)

$$\underset{SEE}{\text{minimize}}  = \sum_{i = 1}^n{(y_i - \hat{y_i})^2} + \lambda(\lvert \beta \lvert)$$
* **Elatic-net:** 1 > alpha > 0, intermedio

# Comparemos en con hp como en la clase anterior

## Ejemplo

```{r}
library(glmnet)
x <- model.matrix(mpg ~., data = mtcars)
x <- x[,-1]
y <- mtcars$mpg
```

```{r}
set.seed(2020)
ridge <- cv.glmnet(x, y, alpha = 0, nfolds = 10)
```

```{r}
set.seed(2020)
lasso <- cv.glmnet(x, y, alpha = 1, nfolds = 10)
```

## Ver crossvalidation lasso

```{r}
plot(lasso)
```

## Variables

```{r, echo = FALSE}
DF <- lasso$glmnet.fit$beta %>% as.matrix() %>% t() %>% as.data.frame()

DF$Lambda <- lasso$glmnet.fit$lambda

DF <- DF %>% pivot_longer(-Lambda, names_to = "Parametro", values_to = "Estimador")

ggplot(DF, aes(x = Lambda, y = Estimador, color = Parametro)) + geom_path() + geom_vline(xintercept = lasso$lambda.min, color = "red", lty = 2) + theme_bw() 
```

## Parametros

```{r, echo = FALSE}
DF %>% dplyr::filter(Lambda == lasso$lambda.min) %>% kable() %>% kable_paper("hover")
```

# Ridge

## Ver crossvalidation lasso

```{r}
plot(ridge)
```

## Variables

```{r, echo = FALSE}
DF <- ridge$glmnet.fit$beta %>% as.matrix() %>% t() %>% as.data.frame()

DF$Lambda <- ridge$glmnet.fit$lambda

DF <- DF %>% pivot_longer(-Lambda, names_to = "Parametro", values_to = "Estimador")

ggplot(DF, aes(x = Lambda, y = Estimador, color = Parametro)) + geom_path() + geom_vline(xintercept = ridge$lambda.min, color = "red", lty = 2) + theme_bw() + scale_x_log10()
```

## Parametros

```{r, echo = FALSE}
DF %>% dplyr::filter(Lambda == ridge$lambda.min) %>% kable() %>% kable_paper("hover")
```


## Mejor modelo

```{r}
Mejor_Lasso <- glmnet(x, y,alpha = 1, lambda = lasso$lambda.min)
```

```{r}
Mejor_Ridge <- glmnet(x, y,alpha = 0, lambda = ridge$lambda.min)
```

## Tidy lasso

```{r, echo = FALSE}
tidy(Mejor_Lasso) %>% kable() %>% kable_paper("hover")
```

## Tidy ridge

```{r, echo = FALSE}
tidy(Mejor_Ridge) %>% kable() %>% kable_paper("hover")
```

# Tiene las mismas familias que glm

## Binomial 

```{r}
train <-  read_csv("https://raw.githubusercontent.com/derek-corcoran-barrios/derek-corcoran-barrios.github.io/master/CursoMultiPres/Capitulo_3/train.csv") 
train <- train[complete.cases(train),]
```

```{r}
x <- model.matrix(Survived ~., data = train)
x <- x[,-1]
y <- train$Survived
set.seed(2020)
Fit <- cv.glmnet(x, y , nfolds = 10, family = "binomial", alpha = 1)
```

## Mejor modelo

```{r}
mejor.Fit <- glmnet(x, y, family = "binomial", alpha = 1, lamda = Fit$lambda.min)
```

```{r}
tidy(mejor.Fit)
```




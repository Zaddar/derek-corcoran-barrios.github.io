---
title: "Regresión Penalizada"
author: "Derek Corcoran"
date: "`r format(Sys.time(), '%d/%m, %Y')`"
output:
  ioslides_presentation:
    widescreen: true
    incremental: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, tidy = TRUE, cache = F)
library(MuMIn)
library(tidyverse)
library(broom)
library(caret)
library(kableExtra)
library(glmnet)
library(patchwork)
options("kableExtra.html.bsTable" = T)
```

## Regresión penalizada

Empezemos con una regresión lineal simple:

```{r}
data("mtcars")
Fit <- lm(mpg ~ wt, data = mtcars)
```

```{r, echo = FALSE}
kable(broom::tidy(Fit), digits = 3) %>% kable_styling(bootstrap_options = c("striped", "hover"))
```

## Como obtenemos la pendiente y el intercepto? {.build}

* Usando mínimos cuadrados

$$\underset{SEE}{\text{minimize}}  = \sum_{i = 1}^n{(y_i - \hat{y_i})^2}$$

## Ejemplo:

```{r, echo = F, animation.hook="gifski", interval = 0.1}

#MinSq <- data.frame(Beta = seq(0, -10, length.out = 100), MinSq = NA) %>% arrange(Beta)
#mt <- mtcars

#for(i in 1:nrow(MinSq)){
#  mt$pred <- 30.1 + MinSq$Beta[i]*mt$wt
#MinSq$MinSq[i] <- mt %>% mutate(resid_sq = (mpg - pred)^2) %>% summarise(MinSq = sum(resid_sq)) %>% pull(MinSq)

#G1 <- ggplot(mt, aes(x = wt, y = mpg)) + geom_path(aes(y = pred)) + geom_linerange(aes(ymin = pred, ymax = mpg)) + geom_point() + theme_bw()

#G2 <- ggplot(MinSq, aes(x = Beta, y = MinSq)) + geom_path() + ylim(c(400,17000))+ theme_bw() + geom_label(aes(x = -5, y = 12000, label = paste0("MinSq =",round(min(MinSq, na.rm = T),0)))) + geom_vline(data =(dplyr::filter(MinSq, MinSq == min(MinSq, na.rm = T))) ,aes(xintercept = Beta), lty = 2, color = "red")
#Final <- G1 + G2
#print(Final)
#}

```

## Cuando esto es un problema

* A veces nuestra poblacion es muy distinta a nuestra muestra

```{r, echo = F}
mt <- mtcars
set.seed(672)
mt_20 <- mt %>% dplyr::sample_frac(0.2)
Fit_20 <- lm(mpg ~ wt, data = mt_20)
  
mt$Pred <- predict(Fit_20, mt)

ggplot(mt, aes(x = wt, y = mpg)) + geom_path(aes(y = Pred)) + geom_point() + geom_point(data = mt_20, color = "red") + theme_bw()
```

## Parametros

```{r}
tidy(Fit_20) %>% kable %>% kable_styling(bootstrap_options = c("striped", "hover"))
```


## Penalizamos con $\lambda$

* Usando penalización

$$\underset{SEE}{\text{minimize}}  = \sum_{i = 1}^n{(y_i - \hat{y_i})^2} + \lambda(\beta_{}^2)$$

## Veamos como cambian los parametros:

```{r, animation.hook="gifski", interval = 0.1, echo = F}
x <- mt_20 %>% dplyr::select(wt) %>% mutate(New = 0) %>% as.matrix()
# Outcome variable
y <- mt_20 %>% pull(mpg)
Fit_Pen <- glmnet(x, y, alpha = 0)
DF <- Fit_Pen$beta %>% as.matrix() %>% t() %>% as.data.frame()
DF$a0 <- Fit_Pen$a0
DF$Lambda <- Fit_Pen$lambda
DF <- DF %>% filter(Lambda < 300) %>% arrange(Lambda)
for(i in 1:nrow(DF)){
  mt$Pred <- DF$a0[i] + mt$wt*DF$wt[i]
  DF$MinSq[i] <- mt %>% mutate(MinSq = (Pred - mpg)^2) %>% summarise(MinSq = sum(MinSq)) %>% pull(MinSq)
g <- ggplot(mt, aes(x = wt, y = mpg)) + geom_path(aes(y = Pred)) + geom_point() + geom_point(data = mt_20, color = "red") + theme_bw() + ggtitle(paste("Lambda =", round(DF$Lambda[i],2))) + ylim(c(-10, 35))
print(g)
}
```


## Seleccionando Lambda

```{r, echo = FALSE}
ggplot(DF, aes(x = Lambda, y = MinSq)) + geom_path() + theme_bw()
```

```{r, echo = F}
DF %>% dplyr::filter(MinSq == min(MinSq)) %>% select(-New) %>% kable(digits = 2) %>% kable_paper("hover")
```




# Nuestro primer hiperparámetro $\lambda$

## k-fold repeated Crossvalidation

* Repito esto n veces
* 10-repeated-5-fold-crossvalidation = 50 $R^2$

```{r, echo = F}
knitr::include_graphics("nrepeatkfold.gif", dpi = 30)
```

## k-fold repeated Crossvalidation (cont)

```{r, echo = FALSE, cache = TRUE}
set.seed(2018)
ctrl <- trainControl(method = "repeatedcv",number = 5, repeats = 10)

Tests2 <- train(mpg ~ hp, data = mtcars, method = "lm",trControl = ctrl)$resample %>% select(Rsquared,Resample)

kable(Tests2, digits = 3) %>% kable_styling(bootstrap_options = c("striped", "hover")) %>% scroll_box(width = "100%", height = "400px")
```

* $R^2$ = `r mean(Tests2$Rsquared)`

## k-fold repeated Crossvalidation (cont)

```{r, echo=F}
ggplot(Tests2, aes(Rsquared)) + geom_density(fill = "grey") + geom_vline(xintercept = mean(Tests2$Rsquared), lty = 2, color = "red") + theme_classic()
```

# Seleccionando modelos usando k-fold repeated Crossvalidation

## Modelos candidatos:

* $mpg = \beta_1hp + c$
* $mpg = \beta_1hp + \beta_2hp^2 + c$
* $mpg = \beta_1hp + \beta_2hp^2 + \beta_3hp^3 + c$
* $mpg = \beta_1hp + \beta_2hp^2 + \beta_3hp^3 + \beta_4hp^4 + c$
* $mpg = \beta_1hp + \beta_2hp^2 + \beta_3hp^3 + \beta_4hp^4 + \beta_5hp^5 + c$
* $mpg = \beta_1hp + \beta_2hp^2 + \beta_3hp^3 + \beta_4hp^4 + \beta_5hp^5 + \beta_6hp^6 + c$

## Seleccionando por AICc

```{r, echo = TRUE}
data("mtcars")

fit1 <- lm(mpg ~ hp, data = mtcars)
fit2 <- lm(mpg ~ hp + I(hp^2), data = mtcars)
fit3 <- lm(mpg ~ hp + I(hp^2) + I(hp^3), data = mtcars)
fit4 <- lm(mpg ~ hp + I(hp^2) + I(hp^3) + I(hp^4), data = mtcars)
fit5 <- lm(mpg ~ hp + I(hp^2) + I(hp^3) + I(hp^4) + I(hp^5), data = mtcars)
fit6 <- lm(mpg ~ hp + I(hp^2) + I(hp^3) + I(hp^4) + I(hp^5) + I(hp^6), data = mtcars)

models <- list(fit1, fit2, fit3, fit4, fit5, fit6)
SelectedMods <- model.sel(models)

```

## Seleccionando por AICc (cont)

```{r, echo = FALSE}
SelectedMods <- SelectedMods %>% as_tibble() %>% dplyr::select(-df, -logLik) %>% mutate(weight = as.numeric(weight))
kable(SelectedMods, digits = 2) %>% kable_styling(bootstrap_options = c("striped", "hover"))
```

## Seleccionando por n-repeated-K-fold-crossvalidation {.build .small}

* para 1 modelo

```{r, echo = TRUE}
set.seed(2020)
ctrl <- trainControl(method = "repeatedcv",number = 5, repeats = 50)

DF <- train(mpg ~ hp, data = mtcars, method = "lm",trControl = ctrl)$resample 

DF <- DF %>% select(Rsquared,Resample)
```


## Ejercicio resuelto {.small}

```{r, echo = TRUE}
form1 <- "mpg ~ hp"
form2 <- "mpg ~ hp + I(hp^2)"
form3 <- "mpg ~ hp + I(hp^2) + I(hp^3)"
form4 <- "mpg ~ hp + I(hp^2) + I(hp^3) + I(hp^4)"
form5 <- "mpg ~ hp + I(hp^2) + I(hp^3) + I(hp^4) + I(hp^5)"
form6 <- "mpg ~ hp + I(hp^2) + I(hp^3) + I(hp^4) + I(hp^5) + I(hp^6)"

forms <- list(form1, form2, form3, form4, form5, form6)
K = (2:7)

ctrl <- trainControl(method = "repeatedcv",number = 5, repeats = 50)
```

## Continuado {.small .build}

```{r, cache= TRUE, echo = TRUE, tidy.opts= list(blank = FALSE, width.cutoff = 30)}
set.seed(2020)
Tests <- forms %>% map(~train(as.formula(.x), data = mtcars, method = "lm",trControl = ctrl)) %>% map(~as.data.frame(.x$resample)) %>% map(~select(.x, Rsquared))  %>% map(~summarise_all(.x,funs(mean, sd), na.rm = T)) %>% map2(.y = forms,~ mutate(.x, model = .y)) %>% reduce(bind_rows) %>% mutate(K = K) %>% arrange(desc(mean))
```

## Continuado {.small .build}

```{r, echo=FALSE}
kable(Tests, digits = 3) %>% kable_styling(bootstrap_options = c("striped", "hover"))
```

